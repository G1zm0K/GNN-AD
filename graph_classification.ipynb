{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Active Directory Environment Classification based on Vulnerability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports and globals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import numpy as np\n",
    "from py2neo import Graph\n",
    "import random\n",
    "import torch\n",
    "from torch_geometric.data import Data, HeteroData\n",
    "import json\n",
    "import os\n",
    "from torch_geometric.nn import to_hetero, GAT, SAGEConv\n",
    "from torch_geometric.explain import Explanation\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Database connection details\n",
    "graph = Graph(\"http://localhost:7474\", auth=(\"neo4j\", \"bloodhoundcommunityedition\"))\n",
    "\n",
    "# Object types and their corresponding properties\n",
    "object_types_and_properties = {\n",
    "    'Domain': ['name', 'objectid', 'highvalue'],\n",
    "    'User': ['name', 'objectid', 'admincount', 'dontreqpreauth', 'pwdneverexpires', 'hasspn', 'highvalue', 'savedcredentials',\n",
    "             'passwordnotreqd', 'pwdlastset', 'lastlogon', 'unconstraineddelegation', 'enabled', 'sensitive'],\n",
    "    'Computer': ['name', 'objectid', 'operatingsystem', 'enabled', 'haslaps', 'highvalue', 'lastlogontimestamp', \n",
    "                 'pwdlastset', 'unconstraineddelegation', 'privesc', 'creddump', \n",
    "                 'exploitable'],\n",
    "    'Group': ['name', 'objectid', 'highvalue', 'admincount'],\n",
    "    'OU': ['name', 'objectid', 'highvalue', 'blocksInheritance'],\n",
    "    'GPO': ['name', 'objectid', 'exploitable'],\n",
    "    'Container': ['name', 'objectid', 'highvalue']\n",
    "}\n",
    "\n",
    "# Relationship types\n",
    "relationship_types = [\n",
    "    'AddMember',\n",
    "    'AddSelf',\n",
    "    'AdminTo',\n",
    "    'AllExtendedRights',\n",
    "    'AllowedToAct',\n",
    "    'AllowedToDelegate',\n",
    "    'CanPSRemote',\n",
    "    'CanRDP',\n",
    "    'Contains',\n",
    "    'ExecuteDCOM',\n",
    "    'ForceChangePassword',\n",
    "    'GenericAll',\n",
    "    'GenericWrite',\n",
    "    'GetChanges',\n",
    "    'GetChangesAll',\n",
    "    'GpLink',\n",
    "    'HasSession',\n",
    "    'MemberOf',\n",
    "    'Owns',\n",
    "    'ReadLAPSPassword',\n",
    "    'SQLAdmin',\n",
    "    'WriteDacl',\n",
    "    'WriteOwner'\n",
    "]\n",
    "\n",
    "# OS possibilities\n",
    "global_os_categories = ['Windows Server 2003 Enterprise Edition', 'Windows Server 2008 Datacenter', 'Windows Server 2008 Enterprise', \n",
    "                        'Windows Server 2008 R2 Datacenter', 'Windows Server 2008 R2 Enterprise', 'Windows Server 2008 R2 Standard', \n",
    "                        'Windows Server 2008 Standard', 'Windows Server 2012 Datacenter', 'Windows Server 2012 R2 Datacenter', \n",
    "                        'Windows Server 2012 R2 Standard', 'Windows Server 2012 Standard', 'Windows Server 2016 Datacenter', \n",
    "                        'Windows Server 2016 Standard']\n",
    "\n",
    "# Object property types\n",
    "object_property_types = {\n",
    "    \"Domain\": {\n",
    "        \"Name\": \"string\",\n",
    "        \"Objectid\": \"string\",\n",
    "        \"Highvalue\": \"boolean\"\n",
    "    },\n",
    "    \"User\": {\n",
    "        \"Name\": \"string\",\n",
    "        \"Objectid\": \"string\",\n",
    "        \"Admincount\": \"boolean\",\n",
    "        \"Dontreqpreauth\": \"boolean\",\n",
    "        \"Pwdneverexpires\": \"boolean\",\n",
    "        \"Hasspn\": \"boolean\",\n",
    "        \"Highvalue\": \"boolean\",\n",
    "        \"Savedcredentials\": \"boolean\",\n",
    "        \"Passwordnotreqd\": \"boolean\",\n",
    "        \"Pwdlastset\": \"numerical\",\n",
    "        \"Lastlogon\": \"numerical\",\n",
    "        \"Unconstraineddelegation\": \"boolean\",\n",
    "        \"Enabled\": \"boolean\",\n",
    "        \"Sensitive\": \"boolean\"\n",
    "    },\n",
    "    \"Computer\": {\n",
    "        \"Name\": \"string\",\n",
    "        \"Objectid\": \"string\",\n",
    "        \"Operatingsystem\": \"categorical\",\n",
    "        \"Enabled\": \"boolean\",\n",
    "        \"Haslaps\": \"boolean\",\n",
    "        \"Highvalue\": \"boolean\",\n",
    "        \"Lastlogontimestamp\": \"numerical\",\n",
    "        \"Pwdlastset\": \"numerical\",\n",
    "        \"Unconstraineddelegation\": \"boolean\",\n",
    "        \"Privesc\": \"boolean\",\n",
    "        \"Creddump\": \"boolean\",\n",
    "        \"Exploitable\": \"boolean\"\n",
    "    },\n",
    "    \"Group\": {\n",
    "        \"Name\": \"string\",\n",
    "        \"Objectid\": \"string\",\n",
    "        \"Highvalue\": \"boolean\",\n",
    "        \"Admincount\": \"boolean\"\n",
    "    },\n",
    "    \"OU\": {\n",
    "        \"Name\": \"string\",\n",
    "        \"Objectid\": \"string\",\n",
    "        \"Highvalue\": \"boolean\",\n",
    "        \"Blocksinheritance\": \"boolean\"\n",
    "    },\n",
    "    \"GPO\": {\n",
    "        \"Name\": \"string\",\n",
    "        \"Objectid\": \"string\",\n",
    "        \"Exploitable\": \"boolean\"\n",
    "    },\n",
    "    \"Container\": {\n",
    "        \"Name\": \"string\",\n",
    "        \"Objectid\": \"string\",\n",
    "        \"Highvalue\": \"boolean\"\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for handling graph database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_neo4j_database(session):\n",
    "    # Delete nodes and edges with batching into 10k objects - From DBCreator\n",
    "    total = 1\n",
    "    while total > 0:\n",
    "        result = session.run(\n",
    "            \"MATCH (n) WITH n LIMIT 10000 DETACH DELETE n RETURN count(n)\")\n",
    "        for r in result:\n",
    "            total = int(r['count(n)'])\n",
    "    session.run(\"CALL apoc.schema.assert({},{},true);\")\n",
    "    \n",
    "        # Remove constraint - From DBCreator\n",
    "    for constraint in session.run(\"SHOW CONSTRAINTS\"):\n",
    "        session.run(\"DROP CONSTRAINT {}\".format(constraint['name']))\n",
    "\n",
    "    icount = session.run(\n",
    "        \"SHOW INDEXES YIELD name RETURN count(*)\")\n",
    "    for r in icount:\n",
    "        ic = int(r['count(*)'])\n",
    "            \n",
    "    while ic >0:\n",
    "    \n",
    "        showall = session.run(\n",
    "            \"SHOW INDEXES\")\n",
    "        for record in showall:\n",
    "            name = (record['name'])\n",
    "            session.run(\"DROP INDEX {}\".format(name))\n",
    "        ic = 0\n",
    "        \n",
    "    # Setting constraints\n",
    "    constraints = [\n",
    "            \"CREATE CONSTRAINT FOR (n:Base) REQUIRE n.neo4jImportId IS UNIQUE;\",\n",
    "            \"CREATE CONSTRAINT FOR (n:Domain) REQUIRE n.neo4jImportId IS UNIQUE;\",\n",
    "            \"CREATE CONSTRAINT FOR (n:Computer) REQUIRE n.neo4jImportId IS UNIQUE;\",\n",
    "            \"CREATE CONSTRAINT FOR (n:User) REQUIRE n.neo4jImportId IS UNIQUE;\",\n",
    "            \"CREATE CONSTRAINT FOR (n:OU) REQUIRE n.neo4jImportId IS UNIQUE;\",\n",
    "            \"CREATE CONSTRAINT FOR (n:GPO) REQUIRE n.neo4jImportId IS UNIQUE;\",\n",
    "            \"CREATE CONSTRAINT FOR (n:Compromised) REQUIRE n.neo4jImportId IS UNIQUE;\",\n",
    "            \"CREATE CONSTRAINT FOR (n:Group) REQUIRE n.neo4jImportId IS UNIQUE;\",\n",
    "            \"CREATE CONSTRAINT FOR (n:Container) REQUIRE n.neo4jImportId IS UNIQUE;\",\n",
    "    ]\n",
    "\n",
    "    for constraint in constraints:\n",
    "        try:\n",
    "            session.run(constraint)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    session.run(\"match (a) -[r] -> () delete a, r\")\n",
    "    session.run(\"match (a) delete a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_from_json(session, file_path):\n",
    "    \"\"\"\n",
    "    Loads a graph from a JSON file into Neo4j.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        query = f\"PROFILE CALL apoc.periodic.iterate(\\\"CALL apoc.import.json('{file_path}')\\\", \\\"RETURN 1\\\", {{batchSize:1000}})\"\n",
    "        session.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Functions for extracting features and creating dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Extract features from neo4j database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract features from the Neo4j database for a specific object type and returns a Pandas DataFrame.\n",
    "def extract_features(graph, labels, properties):\n",
    "\n",
    "    # Create the RETURN clause dynamically based on the provided properties\n",
    "    return_clause = \", \".join([f\"n.{prop} AS node_{prop}\" for prop in properties])\n",
    "\n",
    "    # Define the Cypher query with labels and properties\n",
    "    query = f\"\"\"\n",
    "    MATCH (n:{labels})\n",
    "    RETURN \n",
    "        id(n) AS node_id, \n",
    "        {return_clause}\n",
    "    \"\"\"\n",
    "\n",
    "    # Execute the query and store the results in a Pandas DataFrame\n",
    "    result = graph.run(query)\n",
    "    df = pd.DataFrame(result)\n",
    "\n",
    "    if df.empty:  # Check if the DataFrame is empty\n",
    "        df = pd.DataFrame(columns=['Node ID'] + [prop.title() for prop in properties]) \n",
    "\n",
    "    # Add headers to the DataFrame (adjust based on properties)\n",
    "    df.columns = ['Node ID'] + [prop.title() for prop in properties]\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_node_properties(graph, label):\n",
    "    query = f\"\"\"\n",
    "    MATCH (n:{label})\n",
    "    WITH keys(n) AS keys\n",
    "    UNWIND keys AS key\n",
    "    RETURN DISTINCT key\n",
    "    \"\"\"\n",
    "    result = graph.run(query)\n",
    "    return [record[\"key\"] for record in result]\n",
    "\n",
    "all_possible_object_types_and_properties = {\n",
    "    label: get_node_properties(graph, label) \n",
    "    for label in ['Domain', 'User', 'Computer', 'Group', 'GPO', 'Container']\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to extract relationships from the Neo4j database and returns a Pandas DataFrame.\n",
    "def extract_relationships(graph, rel_types):\n",
    "    \n",
    "    # List to store DataFrames for each relationship type\n",
    "    dfs = []\n",
    "\n",
    "    for rel_type in rel_types:\n",
    "        # Define the Cypher query with dynamic relationship type\n",
    "        query = f\"\"\"\n",
    "        MATCH (source)-[r:{rel_type}]->(target)\n",
    "        RETURN \n",
    "            id(source) AS source_id,\n",
    "            id(target) AS target_id,\n",
    "            TYPE(r) AS relationship_type\n",
    "        \"\"\"\n",
    "\n",
    "        # Execute the query and store the results in a Pandas DataFrame\n",
    "        result = graph.run(query)\n",
    "        df = pd.DataFrame(result)\n",
    "        \n",
    "        if not df.empty:\n",
    "\n",
    "            # Add headers to the DataFrame\n",
    "            df.columns = ['Source ID', 'Target ID', 'Relationship Type']\n",
    "\n",
    "            dfs.append(df)\n",
    "\n",
    "    # Concatenate all DataFrames into a single DataFrame\n",
    "    return pd.concat(dfs, ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Filter dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataframes(dfs):\n",
    "    for object_name in dfs:\n",
    "        for property in dfs[object_name].columns:\n",
    "            # Take first element if it is a list\n",
    "            dfs[object_name][property] = dfs[object_name][property].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "            # Set boolean to False if null\n",
    "            dfs[object_name][property] = dfs[object_name][property].apply(lambda x: False if x == \"null\" else x)\n",
    "    return dfs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def create_tensors_from_dataframe(df, object_type, object_property_types, missing_features):\n",
    "    node_ids = df['Node ID'].values\n",
    "    all_property_values = []\n",
    "    for column in df.columns:\n",
    "        if column in object_property_types[object_type] and column not in missing_features.get(object_type, []):\n",
    "            property_type = object_property_types[object_type][column]\n",
    "            if property_type == 'boolean':\n",
    "                # Convert to 1s and 0s\n",
    "                all_property_values.append(df[column].astype(int).values)  \n",
    "            elif property_type == 'categorical':\n",
    "                # One-hot encoding\n",
    "                one_hot_df = pd.get_dummies(df[column], dtype=int)\n",
    "                # Add missing categories with 0s\n",
    "                for category in global_os_categories:\n",
    "                    if category not in one_hot_df.columns:\n",
    "                        one_hot_df[category] = 0 \n",
    "\n",
    "                # Ensure consistent order of columns\n",
    "                one_hot_df = one_hot_df[global_os_categories]  \n",
    "\n",
    "                for col in one_hot_df.columns:\n",
    "                    all_property_values.append(one_hot_df[col].values)\n",
    "\n",
    "            elif property_type == 'numerical':\n",
    "                # Fill NaN with 0\n",
    "                numeric_values = pd.to_numeric(df[column], errors='coerce').fillna(0).values.reshape(-1, 1)  # Reshape for scaler\n",
    "\n",
    "                # Apply StandardScaler\n",
    "                scaler = StandardScaler()\n",
    "                scaled_values = scaler.fit_transform(numeric_values)\n",
    "\n",
    "                all_property_values.append(scaled_values.flatten())  # Flatten back to 1D\n",
    "            elif property_type == 'string':\n",
    "                # Ignore string columns\n",
    "                pass  \n",
    "            else:\n",
    "                raise ValueError(f\"Unknown property type '{property_type}' for column '{column}'\")\n",
    "        else:\n",
    "            pass\n",
    "    return node_ids, torch.tensor(all_property_values, dtype=torch.float).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to determine the object type based on the Node ID\n",
    "def get_object_type(node_id, source_id_maps):\n",
    "    for obj_type, ids in source_id_maps.items():\n",
    "        if node_id in ids:\n",
    "            return obj_type\n",
    "    #raise ValueError(f\"Node ID {node_id} not found in any object type\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clear_neo4j_database(session):\n",
    "    # Delete nodes and edges with batching into 10k objects - From DBCreator\n",
    "    total = 1\n",
    "    while total > 0:\n",
    "        result = session.run(\n",
    "            \"MATCH (n) WITH n LIMIT 10000 DETACH DELETE n RETURN count(n)\")\n",
    "        for r in result:\n",
    "            total = int(r['count(n)'])\n",
    "    session.run(\"CALL apoc.schema.assert({},{},true);\")\n",
    "    \n",
    "        # Remove constraint - From DBCreator\n",
    "    for constraint in session.run(\"SHOW CONSTRAINTS\"):\n",
    "        session.run(\"DROP CONSTRAINT {}\".format(constraint['name']))\n",
    "\n",
    "    icount = session.run(\n",
    "        \"SHOW INDEXES YIELD name RETURN count(*)\")\n",
    "    for r in icount:\n",
    "        ic = int(r['count(*)'])\n",
    "            \n",
    "    while ic >0:\n",
    "    \n",
    "        showall = session.run(\n",
    "            \"SHOW INDEXES\")\n",
    "        for record in showall:\n",
    "            name = (record['name'])\n",
    "            session.run(\"DROP INDEX {}\".format(name))\n",
    "        ic = 0\n",
    "        \n",
    "    # Setting constraints\n",
    "    constraints = [\n",
    "            \"CREATE CONSTRAINT FOR (n:Base) REQUIRE n.neo4jImportId IS UNIQUE;\",\n",
    "            \"CREATE CONSTRAINT FOR (n:Domain) REQUIRE n.neo4jImportId IS UNIQUE;\",\n",
    "            \"CREATE CONSTRAINT FOR (n:Computer) REQUIRE n.neo4jImportId IS UNIQUE;\",\n",
    "            \"CREATE CONSTRAINT FOR (n:User) REQUIRE n.neo4jImportId IS UNIQUE;\",\n",
    "            \"CREATE CONSTRAINT FOR (n:OU) REQUIRE n.neo4jImportId IS UNIQUE;\",\n",
    "            \"CREATE CONSTRAINT FOR (n:GPO) REQUIRE n.neo4jImportId IS UNIQUE;\",\n",
    "            \"CREATE CONSTRAINT FOR (n:Compromised) REQUIRE n.neo4jImportId IS UNIQUE;\",\n",
    "            \"CREATE CONSTRAINT FOR (n:Group) REQUIRE n.neo4jImportId IS UNIQUE;\",\n",
    "            \"CREATE CONSTRAINT FOR (n:Container) REQUIRE n.neo4jImportId IS UNIQUE;\",\n",
    "    ]\n",
    "\n",
    "    for constraint in constraints:\n",
    "        try:\n",
    "            session.run(constraint)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    session.run(\"match (a) -[r] -> () delete a, r\")\n",
    "    session.run(\"match (a) delete a\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_graph_from_json(session, file_path):\n",
    "    \"\"\"\n",
    "    Loads a graph from a JSON file into Neo4j.\n",
    "    \"\"\"\n",
    "    with open(file_path, 'r') as f:\n",
    "        query = f\"PROFILE CALL apoc.periodic.iterate(\\\"CALL apoc.import.json('{file_path}')\\\", \\\"RETURN 1\\\", {{batchSize:1000}})\"\n",
    "        session.run(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create PyG heterogenous dataset from Pandas DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_heterogeneous_graph(object_dfs, relationship_df, object_property_types, missing_features):\n",
    "    \n",
    "    # Initialize empty objects\n",
    "    data = HeteroData()\n",
    "    source_id_maps = {}\n",
    "    skipped_edges = 0\n",
    "    skipped_nodes = 0\n",
    "\n",
    "    # Add Nodes and Features\n",
    "    for object_name in object_dfs:\n",
    "        node_ids, data[object_name].x = create_tensors_from_dataframe(object_dfs[object_name], object_name, object_property_types, missing_features)\n",
    "        source_id_maps[object_name] = node_ids  \n",
    "\n",
    "    # Add Edges\n",
    "    edge_index_dict = {}  # Dictionary to store edge indices for each edge type\n",
    "    for _, row in relationship_df.iterrows():\n",
    "        try:\n",
    "            source_id, target_id, rel_type = row\n",
    "            source_type = get_object_type(source_id, source_id_maps)\n",
    "            target_type = get_object_type(target_id, source_id_maps)\n",
    "            source_index = np.where(np.isin(source_id_maps[source_type], source_id))[0]  \n",
    "            target_index = np.where(np.isin(source_id_maps[target_type], target_id))[0]\n",
    "\n",
    "            # source_index = np.where(source_id_maps[source_type] == source_id)[0]\n",
    "            # target_index = np.where(source_id_maps[target_type] == target_id)[0]\n",
    "\n",
    "            # Store edge indices in the dictionary\n",
    "            edge_type = (source_type, rel_type, target_type)\n",
    "            if edge_type not in edge_index_dict:\n",
    "                edge_index_dict[edge_type] = [[], []]  # Initialize with empty lists\n",
    "            edge_index_dict[edge_type][0].extend(source_index)\n",
    "            edge_index_dict[edge_type][1].extend(target_index)\n",
    "\n",
    "        except ValueError as e:\n",
    "            skipped_edges += 1\n",
    "        except KeyError as e:\n",
    "            skipped_nodes += 1\n",
    "\n",
    "    # Create edge_index tensors from the accumulated indices\n",
    "    for edge_type, (source_indices, target_indices) in edge_index_dict.items():\n",
    "        source_type, rel_type, target_type = edge_type\n",
    "        data[source_type, rel_type, target_type].edge_index = torch.tensor([source_indices, target_indices], dtype=torch.long)\n",
    "\n",
    "    print(f\"{skipped_edges} edges skipped\")\n",
    "    print(f\"{skipped_nodes} nodes skipped\")\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "clear_neo4j_database(graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_dfs = {}\n",
    "relationship_df = pd.DataFrame()\n",
    "\n",
    "for object_type, properties in object_types_and_properties.items():\n",
    "    # object_dfs[object_type] = extract_features(graph, object_type, properties).dropna()\n",
    "    imputer = SimpleImputer(strategy='most_frequent')  # Or 'median', 'most_frequent'\n",
    "    object_dfs[object_type] = extract_features(graph, object_type, properties)\n",
    "\n",
    "    print(f\"Object dataframe for {object_type}: {object_dfs[object_type]}\")\n",
    "\n",
    "    for property in object_dfs[object_type].columns:\n",
    "        # Take first element if it is a list\n",
    "        object_dfs[object_type][property] = object_dfs[object_type][property].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "        # Set boolean to False if null\n",
    "        object_dfs[object_type][property] = object_dfs[object_type][property].apply(lambda x: False if x == \"null\" or x is None else x) \n",
    "    if not object_dfs[object_type].empty:\n",
    "        object_dfs[object_type][:] = imputer.fit_transform(object_dfs[object_type])  # Impute in place\n",
    "\n",
    "relationship_df = extract_relationships(graph, relationship_types)\n",
    "\n",
    "#print(\"Filtering dataframes\")\n",
    "#object_dfs_filtered = filter_dataframes(object_dfs)\n",
    "\n",
    "# Identify missing features\n",
    "missing_features = {\n",
    "    object_type: list(set(object_types_and_properties[object_type]) - set(df.columns))\n",
    "    for object_type, df in object_dfs.items()\n",
    "}\n",
    "\n",
    "print(f\"Missing features: {missing_features}\")\n",
    "\n",
    "print(\"Creating heterogenous PyG graph\")\n",
    "real_graph_data_rtm_vuln = create_heterogeneous_graph(object_dfs, relationship_df, object_property_types, missing_features)\n",
    "real_graph_data_rtm_vuln.y = torch.tensor([1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vulnerable environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for ADSynth datasets\n",
    "data_dir = \"\"\n",
    "\n",
    "vulnerable_dataset = []\n",
    "\n",
    "#Main generation function\n",
    "print(\"===== Start =====\")\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        print(\"----- Starting process for new graph -----\")\n",
    "        print(f\"Now processing: {filename} \")\n",
    "\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        print(\"Clearing database\")\n",
    "        clear_neo4j_database(graph)\n",
    "\n",
    "        print(\"Loading json file\")\n",
    "        load_graph_from_json(graph, file_path)\n",
    "\n",
    "        object_dfs = {}\n",
    "        relationship_df = pd.DataFrame()\n",
    "        for object_type, properties in object_types_and_properties.items():\n",
    "            # object_dfs[object_type] = extract_features(graph, object_type, properties).dropna()\n",
    "            imputer = SimpleImputer(strategy='most_frequent')  # Or 'median', 'most_frequent'\n",
    "            object_dfs[object_type] = extract_features(graph, object_type, properties)\n",
    "            for property in object_dfs[object_type].columns:\n",
    "                # Take first element if it is a list\n",
    "                object_dfs[object_type][property] = object_dfs[object_type][property].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "                # Set boolean to False if null\n",
    "                object_dfs[object_type][property] = object_dfs[object_type][property].apply(lambda x: False if x == \"null\" or x is None else x) \n",
    "            object_dfs[object_type][:] = imputer.fit_transform(object_dfs[object_type])  # Impute in place\n",
    "\n",
    "        relationship_df = extract_relationships(graph, relationship_types)\n",
    "\n",
    "        #print(\"Filtering dataframes\")\n",
    "        #object_dfs_filtered = filter_dataframes(object_dfs)\n",
    "\n",
    "        print(\"Creating heterogenous PyG graph\")\n",
    "\n",
    "        missing_features = {\n",
    "            object_type: list(set(object_types_and_properties[object_type]) - set(df.columns))\n",
    "            for object_type, df in object_dfs.items()\n",
    "        }\n",
    "\n",
    "        graph_data = create_heterogeneous_graph(object_dfs, relationship_df, object_property_types, missing_features)\n",
    "        graph_data.y = torch.tensor([1])\n",
    "\n",
    "        vulnerable_dataset.append(graph_data)\n",
    "        \n",
    "torch.save(vulnerable_dataset, \"vulnerable_dataset3.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Safe environments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Directory for ADSynth datasets\n",
    "data_dir = \"\"\n",
    "\n",
    "safe_dataset = []\n",
    "\n",
    "#Main generation function\n",
    "print(\"===== Start =====\")\n",
    "for filename in os.listdir(data_dir):\n",
    "    if filename.endswith(\".json\"):\n",
    "        print(\"----- Starting process for new graph -----\")\n",
    "        print(f\"Now processing: {filename} \")\n",
    "\n",
    "        file_path = os.path.join(data_dir, filename)\n",
    "        print(\"Clearing database\")\n",
    "        clear_neo4j_database(graph)\n",
    "\n",
    "        print(\"Loading json file\")\n",
    "        load_graph_from_json(graph, file_path)\n",
    "\n",
    "        object_dfs = {}\n",
    "        relationship_df = pd.DataFrame()\n",
    "        for object_type, properties in object_types_and_properties.items():\n",
    "            # object_dfs[object_type] = extract_features(graph, object_type, properties).dropna()\n",
    "            imputer = SimpleImputer(strategy='most_frequent')  # Or 'median', 'most_frequent'\n",
    "            object_dfs[object_type] = extract_features(graph, object_type, properties)\n",
    "            for property in object_dfs[object_type].columns:\n",
    "                # Take first element if it is a list\n",
    "                object_dfs[object_type][property] = object_dfs[object_type][property].apply(lambda x: x[0] if isinstance(x, list) else x)\n",
    "                # Set boolean to False if null\n",
    "                object_dfs[object_type][property] = object_dfs[object_type][property].apply(lambda x: False if x == \"null\" or x is None else x) \n",
    "            object_dfs[object_type][:] = imputer.fit_transform(object_dfs[object_type])  # Impute in place\n",
    "\n",
    "        relationship_df = extract_relationships(graph, relationship_types)\n",
    "\n",
    "        #print(\"Filtering dataframes\")\n",
    "        #object_dfs_filtered = filter_dataframes(object_dfs)\n",
    "\n",
    "        print(\"Creating heterogenous PyG graph\")\n",
    "\n",
    "        missing_features = {\n",
    "            object_type: list(set(object_types_and_properties[object_type]) - set(df.columns))\n",
    "            for object_type, df in object_dfs.items()\n",
    "        }\n",
    "\n",
    "        graph_data = create_heterogeneous_graph(object_dfs, relationship_df, object_property_types, missing_features)\n",
    "        graph_data.y = torch.tensor([0])\n",
    "\n",
    "        safe_dataset.append(graph_data)\n",
    "        \n",
    "torch.save(safe_dataset, \"safe_dataset3.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.transforms import BaseTransform\n",
    "\n",
    "class CustomAddSelfLoops(BaseTransform):\n",
    "    def __init__(self, attr='edge_weight', fill_value=1.0):\n",
    "        self.attr = attr\n",
    "        self.fill_value = fill_value\n",
    "\n",
    "    def forward(self, data):\n",
    "        for store in data.edge_stores:\n",
    "            if store.is_bipartite() or 'edge_index' not in store:\n",
    "                continue\n",
    "\n",
    "            # Get the number of nodes for the current node type\n",
    "            num_nodes = store.size(0)\n",
    "\n",
    "            # Create self-loop edges (connect each node to itself)\n",
    "            self_loop_edges = torch.arange(num_nodes).repeat(2, 1) \n",
    "\n",
    "            # Concatenate with existing edges\n",
    "            store.edge_index = torch.cat([store.edge_index, self_loop_edges], dim=1)\n",
    "\n",
    "            # Add edge attributes (if needed)\n",
    "            if self.attr is not None:\n",
    "                # Adjust this based on how you want to fill edge attributes for self-loops\n",
    "                self_loop_attr = torch.full((num_nodes,), self.fill_value, dtype=torch.float)\n",
    "                if store.get(self.attr) is not None:\n",
    "                    store[self.attr] = torch.cat([store[self.attr], self_loop_attr])\n",
    "                else:\n",
    "                    store[self.attr] = self_loop_attr\n",
    "        return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import AddSelfLoops\n",
    "\n",
    "vulnerable_dataset = torch.load(\"vulnerable_dataset3.pt\")\n",
    "safe_dataset = torch.load(\"safe_dataset3.pt\")\n",
    "\n",
    "dataset = vulnerable_dataset + safe_dataset\n",
    "\n",
    "# Example usage:\n",
    "# transform = CustomAddSelfLoops()\n",
    "# dataset = [transform(data) for data in dataset]\n",
    "\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Graph has structure: {dataset[0]}')\n",
    "print(f'Number of features: {dataset[0].num_features}')\n",
    "print(f'Number of classes: {len(torch.unique(torch.cat([data.y for data in dataset])))}') \n",
    "print(f'Has isolated nodes: {dataset[0].has_isolated_nodes()}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "# Assuming your dataset has a 'y' attribute for labels\n",
    "labels = [data.y for data in dataset] \n",
    "\n",
    "# Stratified split into train and temp (val + test)\n",
    "train_dataset, temp_dataset, train_labels, temp_labels = train_test_split(\n",
    "    dataset, labels, test_size=0.2, stratify=labels, random_state=1\n",
    ")\n",
    "\n",
    "# Stratified split of temp into val and test\n",
    "val_dataset, test_dataset, val_labels, test_labels = train_test_split(\n",
    "    temp_dataset, temp_labels, test_size=0.5, stratify=temp_labels, random_state=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualising data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from collections import Counter  # Import Counter for top-k analysis\n",
    "\n",
    "def visualize_graph_properties(dataset, top_k_edges=10):  # Add top_k_edges parameter\n",
    "    # --- Separate Data by Class ---\n",
    "    vulnerable_data = [data for data in dataset if data.y.item() == 1]\n",
    "    safe_data = [data for data in dataset if data.y.item() == 0]\n",
    "\n",
    "    # --- Node Count Distribution (Separate Plots for Each Class) ---\n",
    "    node_counts_by_type = defaultdict(lambda: {0: [], 1: []})  \n",
    "    for data in dataset:\n",
    "        class_label = data.y.item()\n",
    "        for node_type in data.node_types:\n",
    "            node_counts_by_type[node_type][class_label].append(data[node_type].num_nodes)\n",
    "\n",
    "    for class_label, class_name in enumerate(['Safe', 'Vulnerable']):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for node_type, counts in node_counts_by_type.items():\n",
    "            sns.histplot(counts[class_label], label=f'{node_type} ({class_name})', kde=True, bins=20)\n",
    "        plt.title(f'Node Count Distribution by Node Type ({class_name})')\n",
    "        plt.xlabel('Number of Nodes')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    # --- Edge Count Distribution (Separate Plots for Each Class + Log Scale) ---\n",
    "    edge_counts_by_type = defaultdict(lambda: {0: [], 1: []})\n",
    "    for data in dataset:\n",
    "        class_label = data.y.item()\n",
    "        for edge_type in data.edge_types:\n",
    "            edge_counts_by_type[edge_type][class_label].append(data[edge_type].edge_index.shape[1])\n",
    "\n",
    "    for class_label, class_name in enumerate(['Safe', 'Vulnerable']):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        for edge_type, counts in edge_counts_by_type.items():\n",
    "            sns.histplot(counts[class_label], label=f'{edge_type} ({class_name})', kde=True, bins=20)\n",
    "        plt.title(f'Edge Count Distribution by Edge Type ({class_name}) - Log Scale')\n",
    "        plt.xlabel('Number of Edges')\n",
    "        plt.ylabel('Frequency')\n",
    "        plt.xscale('log')\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # --- Edge Count Distribution (Top-k, Difference between classes) ---\n",
    "    edge_counts_safe = Counter()\n",
    "    for data in safe_data:\n",
    "        for edge_type in data.edge_types:\n",
    "            edge_counts_safe[edge_type] += data[edge_type].edge_index.shape[1]\n",
    "\n",
    "    edge_counts_vulnerable = Counter()\n",
    "    for data in vulnerable_data:\n",
    "        for edge_type in data.edge_types:\n",
    "            edge_counts_vulnerable[edge_type] += data[edge_type].edge_index.shape[1]\n",
    "\n",
    "    # Calculate the difference in counts\n",
    "    edge_count_diff = {\n",
    "        edge_type: edge_counts_vulnerable.get(edge_type, 0) - edge_counts_safe.get(edge_type, 0)\n",
    "        for edge_type in set(edge_counts_safe) | set(edge_counts_vulnerable)\n",
    "    }\n",
    "\n",
    "    # Sort by absolute difference\n",
    "    sorted_edge_count_diff = sorted(edge_count_diff.items(), key=lambda item: abs(item[1]), reverse=True)\n",
    "    top_k_edges_diff = sorted_edge_count_diff[:top_k_edges]\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.barplot(x=[str(edge[0]) for edge in top_k_edges_diff], y=[edge[1] for edge in top_k_edges_diff])\n",
    "    plt.title(f'Top {top_k_edges} Edge Types with Largest Difference in Frequency (Vulnerable - Safe)')\n",
    "    plt.xlabel('Edge Type')\n",
    "    plt.ylabel('Frequency Difference')\n",
    "    plt.xticks(rotation=45, ha='right')\n",
    "    plt.show()\n",
    "\n",
    "    # --- Feature Statistics (Separate Violin Plots for Each Class) ---\n",
    "    all_feature_stats = {0: [], 1: []}\n",
    "    node_types = {0: [], 1: []}\n",
    "    for data in dataset:\n",
    "        class_label = data.y.item()\n",
    "        for node_type, features in data.x_dict.items():\n",
    "            all_feature_stats[class_label].extend(features.reshape(-1).tolist())\n",
    "            node_types[class_label].extend([node_type] * features.numel())\n",
    "\n",
    "    for class_label, class_name in enumerate(['Safe', 'Vulnerable']):\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.violinplot(x=node_types[class_label], y=all_feature_stats[class_label], inner='quartile')\n",
    "        plt.title(f'Feature Value Distribution by Node Type ({class_name})')\n",
    "        plt.xlabel('Node Type')\n",
    "        plt.ylabel('Feature Value')\n",
    "        plt.xticks(rotation=45)\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    # --- Outlier Detection (Box Plot, Separate by Class) ---\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    sns.boxplot(x=node_types[0], y=all_feature_stats[0], color='blue', label='Safe')\n",
    "    sns.boxplot(x=node_types[1], y=all_feature_stats[1], color='red', label='Vulnerable')\n",
    "    plt.title('Box Plot for Feature Outlier Detection by Class')\n",
    "    plt.xlabel('Node Type')\n",
    "    plt.ylabel('Feature Value')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "    # Class Distribution \n",
    "    vulnerable_count = sum(data.y.item() for data in dataset)\n",
    "    safe_count = len(dataset) - vulnerable_count\n",
    "\n",
    "    plt.figure(figsize=(6, 6))\n",
    "    plt.pie([vulnerable_count, safe_count], labels=['Vulnerable', 'Safe'], autopct='%1.1f%%', startangle=140)\n",
    "    plt.title('Class Distribution')\n",
    "    plt.show()\n",
    "\n",
    "    # Example usage:\n",
    "    # Assuming you have a list of `HeteroData` objects called `dataset`\n",
    "visualize_graph_properties(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import networkx as nx\n",
    "\n",
    "def visualize_heterogeneous_graph(data, node_type_colors):\n",
    "    \"\"\"\n",
    "    Visualizes a heterogeneous graph using networkx and matplotlib.\n",
    "\n",
    "    Args:\n",
    "        data (HeteroData): The heterogeneous graph data.\n",
    "        node_type_colors (dict): A dictionary mapping node types to colors.\n",
    "    \"\"\"\n",
    "\n",
    "    G = nx.DiGraph()  # Create a directed graph\n",
    "\n",
    "    # Add nodes with labels and colors\n",
    "    for node_type in data.node_types:\n",
    "        G.add_nodes_from(range(data[node_type].num_nodes), label=node_type)\n",
    "\n",
    "    # Add edges\n",
    "    for edge_type in data.edge_types:\n",
    "        src_type, _, dst_type = edge_type\n",
    "        edge_index = data[edge_type].edge_index\n",
    "        for src, dst in edge_index.t().tolist():\n",
    "            G.add_edge(src, dst)\n",
    "\n",
    "    # Increase figure size\n",
    "    plt.figure(figsize=(150, 150))  # Adjust figsize as needed\n",
    "\n",
    "    # Position nodes using a suitable layout with more space\n",
    "    pos = nx.spring_layout(G, k=0.8, iterations=50)  # Adjust k and iterations\n",
    "\n",
    "    # Draw nodes with colors\n",
    "    for node_type in data.node_types:\n",
    "        nx.draw_networkx_nodes(\n",
    "            G,\n",
    "            pos,\n",
    "            nodelist=[node for node, label in G.nodes(data='label') if label == node_type],\n",
    "            node_color=node_type_colors[node_type],\n",
    "            label=node_type,\n",
    "        )\n",
    "\n",
    "    # Draw edges\n",
    "    nx.draw_networkx_edges(G, pos)\n",
    "\n",
    "    # Add labels to nodes\n",
    "    nx.draw_networkx_labels(G, pos, font_size=8)\n",
    "\n",
    "    plt.title(\"Heterogeneous Graph Visualization\")\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have a `HeteroData` object called `data`\n",
    "\n",
    "# Define colors for each node type\n",
    "node_type_colors = {\n",
    "    'Domain': 'pink',\n",
    "    'User': 'red',\n",
    "    'Computer': 'blue',\n",
    "    'Group': 'green',\n",
    "    'OU': 'purple',\n",
    "    'GPO': 'orange',\n",
    "    'Container': 'yellow',\n",
    "}\n",
    "\n",
    "# Visualize the graph\n",
    "visualize_heterogeneous_graph(train_dataset[2], node_type_colors)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GNN "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAGEConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import SAGEConv, HeteroConv, GATConv\n",
    "\n",
    "class HeterogeneousGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, metadata):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define convolutional layers for each edge type\n",
    "        self.conv = HeteroConv({\n",
    "            edge_type: SAGEConv((-1, -1), hidden_channels)\n",
    "            for edge_type in metadata[1]\n",
    "        }, aggr='mean')\n",
    "\n",
    "        # Linear layer for final classification\n",
    "        self.lin = torch.nn.Linear(hidden_channels*len(metadata[0]), 1)  # 2 output classes\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "\n",
    "        # 1. Heterogeneous Convolution\n",
    "        x_dict = self.conv(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
    "\n",
    "        # 2. Aggregation (e.g., mean all node embeddings)\n",
    "        x_list = []\n",
    "        for object_type in x_dict:\n",
    "            x_list.append(x_dict[object_type].mean(dim=0))\n",
    "        x_list\n",
    "        x = torch.cat(x_list, dim=0) \n",
    "\n",
    "        # 3. Linear layer for classification\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # 4. Apply activation function\n",
    "        #x = torch.relu(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAGEConv 2 layers?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HeterogeneousGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, metadata, num_layers=2):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Create multiple convolution layers\n",
    "        self.convs = torch.nn.ModuleList([\n",
    "            HeteroConv({\n",
    "                edge_type: SAGEConv((-1, -1), hidden_channels)\n",
    "                for edge_type in metadata[1]\n",
    "            }, aggr='mean') for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "        # Linear layer for final classification\n",
    "        self.lin = torch.nn.Linear(hidden_channels*len(metadata[0]), 1)\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        # Multi-layer heterogeneous convolution\n",
    "        for conv in self.convs:\n",
    "            x_dict = conv(x_dict, edge_index_dict)\n",
    "            x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
    "        \n",
    "        # Aggregation\n",
    "        x_list = [x_dict[object_type].mean(dim=0) for object_type in x_dict]\n",
    "        x = torch.cat(x_list, dim=0)\n",
    "        \n",
    "        # Linear layer and final activation\n",
    "        x = self.lin(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SAGEConv 2 layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import SAGEConv, HeteroConv, GATConv\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class HeterogeneousGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, metadata):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1 = HeteroConv({\n",
    "            edge_type: SAGEConv((-1, -1), hidden_channels)\n",
    "            for edge_type in metadata[1]\n",
    "        }, aggr='sum')\n",
    "\n",
    "        self.conv2 = HeteroConv({\n",
    "            edge_type: SAGEConv((-1, -1), hidden_channels)\n",
    "            for edge_type in metadata[1]\n",
    "        }, aggr='sum')\n",
    "\n",
    "        self.lin = torch.nn.Linear(hidden_channels*4, 1) \n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        # 1. First Convolution \n",
    "        x_dict = self.conv1(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
    "        x_dict = {key: F.dropout(x, p=0.1, training=self.training) for key, x in x_dict.items()}\n",
    "        print(\"x_dict after conv1:\", {key: x.shape if x is not None else None for key, x in x_dict_conv1.items()})\n",
    "  \n",
    "\n",
    "        # 2. Second Convolution (with handling for no-edge nodes)\n",
    "        x_dict_conv2 = self.conv2(x_dict, edge_index_dict)\n",
    "\n",
    "        for key in x_dict: \n",
    "            if key in x_dict_conv2:\n",
    "                x_dict[key] = x_dict_conv2[key] \n",
    "            else:\n",
    "                # If no output from conv2, keep the original features\n",
    "                x_dict[key] = x_dict[key]  \n",
    "\n",
    "        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
    "        x_dict = {key: F.dropout(x, p=0.1, training=self.training) for key, x in x_dict.items()}  \n",
    "\n",
    "        # 3. Aggregation (include GPO now)\n",
    "        x = torch.cat([x_dict['User'].mean(dim=0), \n",
    "                       x_dict['Computer'].mean(dim=0),\n",
    "                       x_dict['Group'].mean(dim=0),\n",
    "                       x_dict['OU'].mean(dim=0),\n",
    "                       x_dict['GPO'].mean(dim=0)], dim=0)  # Include GPO\n",
    "\n",
    "        # 4. Linear and Activation\n",
    "        x = self.lin(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### GATConv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.nn import SAGEConv, HeteroConv, GATConv\n",
    "\n",
    "class HeterogeneousGNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, metadata):\n",
    "        super().__init__()\n",
    "\n",
    "        # Define convolutional layers for each edge type\n",
    "        self.conv = HeteroConv({\n",
    "            edge_type: GATConv((-1, -1), hidden_channels, add_self_loops=False)\n",
    "            for edge_type in metadata[1]\n",
    "        }, aggr='sum')\n",
    "\n",
    "        # Linear layer for final classification\n",
    "        self.lin = torch.nn.Linear(hidden_channels*4, 1)  # 2 output classes\n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        # 1. Heterogeneous Convolution\n",
    "        x_dict = self.conv(x_dict, edge_index_dict)\n",
    "        x_dict = {key: F.relu(x) for key, x in x_dict.items()}\n",
    "        # Apply dropout to the output of each convolution\n",
    "        #x_dict = {key: F.dropout(x, p=0.9, training=self.training) for key, x in x_dict.items()}  \n",
    "\n",
    "        \n",
    "\n",
    "        # 2. Aggregation (e.g., mean all node embeddings)\n",
    "        x = torch.cat([x_dict['User'].mean(dim=0), \n",
    "                       x_dict['Computer'].mean(dim=0),\n",
    "                       x_dict['Group'].mean(dim=0),\n",
    "                       x_dict['OU'].mean(dim=0),\n",
    "                       #x_dict['GPO'].mean(dim=0)\n",
    "                       ], dim=0)\n",
    "\n",
    "        # 3. Linear layer for classification\n",
    "        x = self.lin(x)\n",
    "\n",
    "        # 4. Apply activation function\n",
    "        #x = torch.relu(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "import itertools\n",
    "from sklearn.metrics import f1_score\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "def plot_learning_curves(history, params, save_dir='hetero_learning_curves'):\n",
    "    \"\"\"Plot and save learning curves for a parameter combination\"\"\"\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    # Loss subplot\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(history['train_losses'], label='Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Training Loss')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Accuracy subplot\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(history['train_accuracies'], label='Training Accuracy')\n",
    "    plt.plot(history['val_accuracies'], label='Validation Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Training and Validation Accuracy')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Create title with parameters\n",
    "    plt.suptitle(f'Hidden: {params[\"hidden_channels\"]}, LR: {params[\"learning_rate\"]}, Layers: {params[\"num_layers\"]}')\n",
    "    \n",
    "    os.makedirs(save_dir, exist_ok=True)\n",
    "    filename = f'hidden_{params[\"hidden_channels\"]}_lr_{params[\"learning_rate\"]}_numlayers_{params[\"num_layers\"]}.png'\n",
    "    plt.savefig(os.path.join(save_dir, filename), bbox_inches='tight', dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "def train_model(model, train_dataset, val_dataset, learning_rate, num_epochs=50, warmup_epochs=10):\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    criterion = torch.nn.BCELoss()\n",
    "    \n",
    "    best_metrics = {\n",
    "        'train_loss': float('inf'),\n",
    "        'train_acc': 0,\n",
    "        'val_acc': 0,\n",
    "        'val_f1': 0,\n",
    "        'epoch': 0,\n",
    "        'is_warmup': True\n",
    "    }\n",
    "    \n",
    "    history = {\n",
    "        'train_losses': [],\n",
    "        'train_accuracies': [],\n",
    "        'val_accuracies': [],\n",
    "        'val_f1_scores': []\n",
    "    }\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        epoch_loss = 0\n",
    "        correct_train = 0\n",
    "        total_train = 0\n",
    "        \n",
    "        for i in range(len(train_dataset)):\n",
    "            data_object = train_dataset[i]\n",
    "            optimizer.zero_grad()\n",
    "            out = model(data_object.x_dict, data_object.edge_index_dict)\n",
    "            y = data_object.y.float()\n",
    "            \n",
    "            loss = criterion(out, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            pred_train = (out > 0.5).float()\n",
    "            correct_train += int((pred_train == y).sum())\n",
    "            total_train += 1\n",
    "        \n",
    "        train_loss = epoch_loss / len(train_dataset)\n",
    "        train_acc = correct_train / total_train\n",
    "        \n",
    "        # Validation\n",
    "        model.eval()\n",
    "        correct_val = 0\n",
    "        total_val = 0\n",
    "        val_predictions = []\n",
    "        val_labels = []\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            for i in range(len(val_dataset)):\n",
    "                data_object = val_dataset[i]\n",
    "                out = model(data_object.x_dict, data_object.edge_index_dict)\n",
    "                y = data_object.y.float()\n",
    "                \n",
    "                pred_val = (out > 0.5).float()\n",
    "                correct_val += int((pred_val == y).sum())\n",
    "                total_val += 1\n",
    "                \n",
    "                val_predictions.extend(pred_val.cpu().numpy())\n",
    "                val_labels.extend(y.cpu().numpy())\n",
    "        \n",
    "        val_acc = correct_val / total_val\n",
    "        val_f1 = f1_score(val_labels, val_predictions, average='binary')\n",
    "        \n",
    "        # Store history\n",
    "        history['train_losses'].append(train_loss)\n",
    "        history['train_accuracies'].append(train_acc)\n",
    "        history['val_accuracies'].append(val_acc)\n",
    "        history['val_f1_scores'].append(val_f1)\n",
    "        \n",
    "        # Update best metrics after warmup\n",
    "        if epoch >= warmup_epochs:\n",
    "            if best_metrics['is_warmup']:\n",
    "                best_metrics = {\n",
    "                    'train_loss': train_loss,\n",
    "                    'train_acc': train_acc,\n",
    "                    'val_acc': val_acc,\n",
    "                    'val_f1': val_f1,\n",
    "                    'epoch': epoch,\n",
    "                    'is_warmup': False\n",
    "                }\n",
    "            else:\n",
    "                if val_f1 > best_metrics['val_f1']:\n",
    "                    best_metrics.update({\n",
    "                        'train_loss': train_loss,\n",
    "                        'train_acc': train_acc,\n",
    "                        'val_acc': val_acc,\n",
    "                        'val_f1': val_f1,\n",
    "                        'epoch': epoch\n",
    "                    })\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "            print(f\"Train - Loss: {train_loss:.4f}, Acc: {train_acc:.4f}\")\n",
    "            print(f\"Val - Acc: {val_acc:.4f}, F1: {val_f1:.4f}\")\n",
    "    \n",
    "    return best_metrics, history\n",
    "\n",
    "def grid_search(train_dataset, val_dataset, metadata, warmup_epochs=10):\n",
    "    param_grid = {\n",
    "        'hidden_channels': [32, 64, 128],\n",
    "        'learning_rate': [0.0001, 0.0005, 0.001],\n",
    "        'num_layers': [1, 2, 3]\n",
    "    }\n",
    "    \n",
    "    param_combinations = list(itertools.product(\n",
    "        param_grid['hidden_channels'],\n",
    "        param_grid['learning_rate'],\n",
    "        param_grid['num_layers']\n",
    "    ))\n",
    "    \n",
    "    results = []\n",
    "    best_overall = {\n",
    "        'train_loss': float('inf'),\n",
    "        'train_acc': 0,\n",
    "        'val_acc': 0,\n",
    "        'val_f1': 0,\n",
    "        'params': None,\n",
    "        'model': None\n",
    "    }\n",
    "    \n",
    "    total_combinations = len(param_combinations)\n",
    "    for idx, (hidden_channels, lr, layers) in enumerate(param_combinations, 1):\n",
    "        print(f\"\\nTesting combination {idx}/{total_combinations}:\")\n",
    "        print(f\"Hidden Channels: {hidden_channels}, Learning Rate: {lr}, Layers: {layers}\")\n",
    "        \n",
    "        model = HeterogeneousGNN(hidden_channels=hidden_channels, metadata=metadata, num_layers=layers)\n",
    "        \n",
    "        best_metrics, history = train_model(\n",
    "            model,\n",
    "            train_dataset,\n",
    "            val_dataset,\n",
    "            learning_rate=lr,\n",
    "            warmup_epochs=warmup_epochs\n",
    "        )\n",
    "        \n",
    "        current_params = {\n",
    "            'hidden_channels': hidden_channels,\n",
    "            'learning_rate': lr,\n",
    "            'num_layers': layers,\n",
    "            'train_loss': best_metrics['train_loss'],\n",
    "            'train_acc': best_metrics['train_acc'],\n",
    "            'val_acc': best_metrics['val_acc'],\n",
    "            'val_f1': best_metrics['val_f1'],\n",
    "            'best_epoch': best_metrics['epoch']\n",
    "        }\n",
    "        \n",
    "        plot_learning_curves(history, current_params)\n",
    "        results.append(current_params)\n",
    "        \n",
    "        # Change metric to val_acc instead of val_f1\n",
    "        if current_params['val_acc'] > best_overall['val_acc']:\n",
    "            best_overall.update({\n",
    "                'train_loss': current_params['train_loss'],\n",
    "                'train_acc': current_params['train_acc'],\n",
    "                'val_acc': current_params['val_acc'],\n",
    "                'val_f1': current_params['val_f1'],\n",
    "                'params': current_params,\n",
    "                'model': model.state_dict()\n",
    "            })\n",
    "        \n",
    "        # Print current best parameters\n",
    "        print(\"\\nCurrent Best Parameters:\")\n",
    "        print(f\"Hidden Channels: {best_overall['params']['hidden_channels']}\")\n",
    "        print(f\"Learning Rate: {best_overall['params']['learning_rate']}\")\n",
    "        print(f\"Number of Layers: {best_overall['params']['num_layers']}\")\n",
    "        print(f\"Validation Accuracy: {best_overall['val_acc']:.4f}\")\n",
    "    \n",
    "    return results, best_overall['params'], best_overall['model']\n",
    "\n",
    "# Example usage remains the same\n",
    "results, best_params, best_model = grid_search(\n",
    "    train_dataset,\n",
    "    val_dataset,\n",
    "    metadata,\n",
    "    warmup_epochs=10\n",
    ")\n",
    "print(\"\\nBest Parameters:\")\n",
    "print(f\"Hidden Channels: {best_params['hidden_channels']}\")\n",
    "print(f\"Learning Rate: {best_params['learning_rate']}\")\n",
    "print(f\"Number of Layers: {best_params['num_layers']}\")\n",
    "print(f\"Training Loss: {best_params['train_loss']:.4f}\")\n",
    "print(f\"Training Accuracy: {best_params['train_acc']:.4f}\")\n",
    "print(f\"Validation Accuracy: {best_params['val_acc']:.4f}\")\n",
    "print(f\"Validation F1 Score: {best_params['val_f1']:.4f}\")\n",
    "print(f\"Best Epoch: {best_params['best_epoch']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.loader import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "hidden_channels = 32\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 40\n",
    "num_layers = 3\n",
    "        \n",
    "model = HeterogeneousGNN(hidden_channels=hidden_channels, metadata=metadata, num_layers=num_layers)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "criterion = torch.nn.BCELoss()\n",
    "\n",
    "\n",
    "# Lists to store loss and accuracy values for plotting\n",
    "train_losses = []\n",
    "train_accuracies = []\n",
    "test_accuracies = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    correct_train = 0\n",
    "    total_train = 0\n",
    "    \n",
    "\n",
    "    for i in range(len(train_dataset)):\n",
    "        data_object = train_dataset[i]\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data_object.x_dict, data_object.edge_index_dict)\n",
    "        y = data_object.y.float()\n",
    "\n",
    "        # --- Debug statements ---\n",
    "        #print(f\"Epoch {epoch+1}, Data point {i+1}\")\n",
    "        #print(f\"  Output shape: {out.shape}, Output values: {out}\")\n",
    "        #print(f\"  Target shape: {y.shape}, Target values: {y}\")\n",
    "        # --- End debug statements ---\n",
    "\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()  # Accumulate loss\n",
    "\n",
    "        # Calculate training accuracy (with thresholding)\n",
    "        pred_train = (out > 0.5).float()  \n",
    "        correct_train += int((pred_train == y).sum())\n",
    "        total_train += 1\n",
    "\n",
    "        # --- Debug statements ---\n",
    "        #print(f\"  Loss: {loss.item():.4f}\")\n",
    "        #for name, param in model.named_parameters():\n",
    "        #    if param.grad is not None:\n",
    "        #        print(f\"  Gradient norm of {name}: {param.grad.norm()}\")\n",
    "        # --- End debug statements ---\n",
    "        \n",
    "    train_accuracy = correct_train / total_train\n",
    "    train_accuracies.append(train_accuracy)\n",
    "    train_losses.append(epoch_loss / len(train_dataset))  # Average loss\n",
    "\n",
    "    # Testing\n",
    "    model.eval()\n",
    "    correct_val = 0\n",
    "    total_val = 0\n",
    "    with torch.no_grad():\n",
    "        for i in range(len(val_dataset)):\n",
    "            data_object = val_dataset[i]\n",
    "            out = model(data_object.x_dict, data_object.edge_index_dict)\n",
    "            y = data_object.y.float()\n",
    "\n",
    "            pred_val = (out > 0.5).float()\n",
    "            correct_val += int((pred_val == y).sum())\n",
    "            total_val += 1 # Or data_object.num_graphs\n",
    "\n",
    "    val_accuracy = correct_val / total_val\n",
    "    test_accuracies.append(val_accuracy)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {epoch_loss:.4f}, \"\n",
    "          f\"Train Acc: {train_accuracy:.4f}, Validation Acc: {val_accuracy:.4f}\")\n",
    "\n",
    "# Plot the loss and accuracy curves\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Training Accuracy')\n",
    "plt.plot(test_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "correct_test = 0\n",
    "total_test = 0\n",
    "with torch.no_grad():\n",
    "    for i in range(len(test_dataset)):  # Iterate over each data point in the test set\n",
    "        data_object = test_dataset[i]\n",
    "        out = model(data_object.x_dict, data_object.edge_index_dict)\n",
    "        y = data_object.y.float().unsqueeze(1)  # Add dimension for single-example \"batch\"\n",
    "\n",
    "        pred_test = (out > 0.5).float()\n",
    "        correct_test += int((pred_test == y).sum())\n",
    "        total_test += 1\n",
    "\n",
    "test_accuracy = correct_test / total_test\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for i in range(len(val_dataset)):\n",
    "        data_object = val_dataset[i]\n",
    "        out = model(data_object.x_dict, data_object.edge_index_dict)\n",
    "        y = data_object.y.float().unsqueeze(1)\n",
    "\n",
    "        pred_test = (out > 0.5).int() # Store as integers for confusion matrix\n",
    "        y_true.extend(y.numpy().flatten().tolist()) # Move to cpu, numpy, flatten, to list\n",
    "        y_pred.extend(pred_test.numpy().flatten().tolist()) # Move to cpu, numpy, flatten, to list\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "y_true = np.array(y_true)\n",
    "y_pred = np.array(y_pred)\n",
    "\n",
    "# Calculate confusion matrix\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Calculate classification report (includes precision, recall, F1-score)\n",
    "cr = classification_report(y_true, y_pred, target_names=['Safe', 'Vulnerable']) # Add target names for better readability\n",
    "print(\"\\nClassification Report:\")\n",
    "print(cr)\n",
    "\n",
    "# Calculate overall accuracy (can cross-check with the report)\n",
    "accuracy = np.sum(y_true == y_pred) / len(y_true)\n",
    "print(f\"\\nOverall Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Extract TP, TN, FP, FN for manual metric calculation if needed\n",
    "TN, FP, FN, TP = cm.ravel()\n",
    "\n",
    "#Manual calculation of metrics for verification if needed\n",
    "precision = TP / (TP + FP)\n",
    "recall = TP / (TP + FN)\n",
    "f1 = 2 * precision * recall / (precision + recall)\n",
    "\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1: {f1:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Real life graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_single_graph(model, graph_data):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        out = model(graph_data.x_dict, graph_data.edge_index_dict)\n",
    "        pred_test = (out > 0.5).float()\n",
    "        if pred_test == 1:\n",
    "            print(f\"This network is vulnerable!\")\n",
    "        elif pred_test == 0:\n",
    "            print(f\"This network is safe!\")\n",
    "            "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
